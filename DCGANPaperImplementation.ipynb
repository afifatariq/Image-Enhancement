{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGANPaperImplementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21cWE4wCqFhf",
        "colab_type": "text"
      },
      "source": [
        "By Afifa Tariq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzmZ0iwPp3Yl",
        "colab_type": "text"
      },
      "source": [
        "Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434eK6oopyyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import itertools\n",
        "import sys\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.autograd import Variable\n",
        "import torchvision.datasets as dset\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCcT8YI7p-K0",
        "colab_type": "text"
      },
      "source": [
        "Define Generator, and Discriminator Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Ji9L3_gccv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_features, in_features, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_block(x)\n",
        "\n",
        "\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=5):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 128, kernel_size=4, stride=1, padding=4), nn.PReLU())\n",
        "        \n",
        "\n",
        "        # Residual blocks\n",
        "        res_blocks = []\n",
        "        for _ in range(n_residual_blocks):\n",
        "            res_blocks.append(ResidualBlock(128))\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), nn.ReLU())\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1), nn.ReLU())\n",
        "\n",
        "        # Final output layer\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=3, stride=1, padding=4), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.res_blocks(out1)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        in_channels, in_height, in_width = self.input_shape\n",
        "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
        "        self.output_shape = (1, patch_h, patch_w)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "            layers = []\n",
        "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            if not first_block:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = in_channels\n",
        "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, in_filters, kernel_size=3, stride=1, padding=1))\n",
        "        layers.append(nn.BatchNorm2d(in_filters))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        layers.append(nn.Conv2d(in_filters, in_filters, kernel_size=3, stride=1, padding=1))\n",
        "        layers.append(nn.BatchNorm2d(in_filters))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.adv_layer = nn.Sequential(*layers, nn.Sigmoid())\n",
        "        # The height and width of downsampled image\n",
        "        #ds_size = 64 // 2 ** 4\n",
        "        #self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, img):\n",
        "        #return self.model(img)\n",
        "\n",
        "        out = self.model(img)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)\n",
        "        print(validity.size())\n",
        "        return validity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrlgXFDZcAdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqv3LZhYuErr",
        "colab_type": "text"
      },
      "source": [
        "Create class for dataset which overrides the dataset class. This class will let me divide the CelebA dataset images into low resolution images and high resolution images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI5IC3RqglIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalization parameters for pre-trained PyTorch models\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, hr_shape):\n",
        "        hr_height, hr_width = hr_shape\n",
        "        # Transforms for low resolution images and high resolution images\n",
        "        self.lr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        self.hr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        img_lr = self.lr_transform(img)\n",
        "        img_hr = self.hr_transform(img)\n",
        "\n",
        "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrTyK6iGwCwT",
        "colab_type": "text"
      },
      "source": [
        "Make folders to save data and define variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ZHuoyBwAEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs(\"images\", exist_ok=True)\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "epoch=0\n",
        "n_epochs=20\n",
        "root = 'data_faces/img_align_celeba'\n",
        "dataset_name=\"img_align_celeba\"\n",
        "batch_size=4\n",
        "lr=0.0002\n",
        "b1=0.5\n",
        "b2=0.999\n",
        "decay_epoch=100\n",
        "n_cpu=2\n",
        "hr_height = 64\n",
        "hr_width = 64\n",
        "channels=3\n",
        "sample_interval=100\n",
        "checkpoint_interval=-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GjZPqonvnNf",
        "colab_type": "text"
      },
      "source": [
        "Initialize Generator and Discriminator\n",
        "Define Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaODE4qRgYyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "hr_shape = (hr_height, hr_width)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = GeneratorResNet()\n",
        "discriminator = Discriminator(input_shape=(channels, *hr_shape))\n",
        "\n",
        "\n",
        "# Losses\n",
        "criterion_GAN = torch.nn.BCELoss()\n",
        "criterion_content = torch.nn.L1Loss()\n",
        "\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    criterion_GAN = criterion_GAN.cuda()\n",
        "    criterion_content = criterion_content.cuda()\n",
        "\n",
        "if epoch != 0:\n",
        "    # Load pretrained models\n",
        "    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"))\n",
        "    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"))\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me29_Jsh1aAW",
        "colab_type": "text"
      },
      "source": [
        "Download CelebA dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWj3P6verQxW",
        "colab_type": "code",
        "outputId": "8d4f5076-176c-4b22-ba1b-78c003159ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip \n",
        "\n",
        "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"data_faces/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-14 19:27:39--  https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.112.41\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.112.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1443490838 (1.3G) [application/zip]\n",
            "Saving to: ‘celeba.zip’\n",
            "\n",
            "celeba.zip          100%[===================>]   1.34G  20.5MB/s    in 67s     \n",
            "\n",
            "2019-12-14 19:28:52 (20.5 MB/s) - ‘celeba.zip’ saved [1443490838/1443490838]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI3vf2k52KKy",
        "colab_type": "text"
      },
      "source": [
        "I used this code to delete some of the image files from the dataset to make it smaller so that the training time would decrease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U10gWcSmJcT",
        "colab_type": "code",
        "outputId": "09cc5379-2d5a-446e-ad50-9695354696c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "\n",
        "files = os.listdir(root)  # Get filenames in current folder\n",
        "files = random.sample(files, 20000)  # Pick 500 random files\n",
        "for file in files:  # Go over each file name to be deleted\n",
        "    f = os.path.join(root, file)  # Create valid path to file\n",
        "    os.remove(f)  # Remove the file\n",
        "\n",
        "img_list = os.listdir(root)\n",
        "print(len(img_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNs5cXdMLEWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgdataset = ImageDataset(root, hr_shape=hr_shape)\n",
        "\n",
        "# Create the dataloader\n",
        "dataloader = DataLoader(\n",
        "    imgdataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8tQNP1x2l-U",
        "colab_type": "text"
      },
      "source": [
        "Mounting my drive to google colab so that I can save the output images and models to the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCAYJUNsa2Qj",
        "colab_type": "code",
        "outputId": "0c278f84-fb17-4948-da8b-8ce752ca1035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LhCCHm029LO",
        "colab_type": "text"
      },
      "source": [
        "Make folders in google drive to save the images and models in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m42nbxnbU-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!mkdir -p \"/content/gdrive/My Drive/images\"\n",
        "!mkdir -p \"/content/gdrive/My Drive/saved_models\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPS8IF7f5OEm",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFtoEsPpmgxq",
        "colab_type": "code",
        "outputId": "052dcc2c-a46f-43d8-b5fa-6d27a5ea4ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(epoch, n_epochs):\n",
        "    for i, imgs in enumerate(dataloader):\n",
        "\n",
        "        # Configure model input\n",
        "        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n",
        "        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generate a high resolution image from low resolution input\n",
        "        gen_hr = generator(imgs_lr)\n",
        "        print(gen_hr.size())\n",
        "\n",
        "        # Adversarial loss\n",
        "        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n",
        "\n",
        "        # Content loss\n",
        "        #gen_features = feature_extractor(gen_hr)\n",
        "        #real_features = feature_extractor(imgs_hr)\n",
        "        loss_content = criterion_content(gen_hr, real_features.detach())\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_content + 1e-3 * loss_GAN\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss of real and fake images\n",
        "        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n",
        "        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(loss_G.item())\n",
        "        D_losses.append(loss_D.item())\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "        if i % 50 == 0:\n",
        "          print(\n",
        "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "              % (epoch, n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n",
        "          )\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % sample_interval == 0:\n",
        "            # Save image grid with upsampled inputs and SRGAN outputs\n",
        "            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
        "            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n",
        "            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n",
        "            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n",
        "            save_image(img_grid, \"/content/gdrive/My Drive/images/%d.png\" % batches_done, normalize=False)\n",
        "\n",
        "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
        "        # Save model checkpoints\n",
        "        torch.save(generator.state_dict(), \"/content/gdrive/My Drive/saved_models/generator_%d.pth\" % epoch)\n",
        "        torch.save(discriminator.state_dict(), \"/content/gdrive/My Drive/saved_models/discriminator_%d.pth\" % epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 678, 678])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOnmkAaqOvNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymLIPRvW8TCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykEsOqrn8oib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage import io\n",
        "\n",
        "img_file = \"16x16-obama.jpg\"\n",
        "img_lr = io.imread(img_file)\n",
        "\n",
        "\n",
        "input_img =torch.tensor(img_lr).float()\n",
        "input_img = Variable(input_img)\n",
        "input_img = np.transpose(input_img, (2, 0, 1))\n",
        "input_img= input_img.unsqueeze(0)\n",
        "# Generate a high resolution image from low resolution input\n",
        "gen_hr = generator(input_img)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}