{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGANimplementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvNsjIOepymT",
        "colab_type": "text"
      },
      "source": [
        "By Afifa Tariq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzmZ0iwPp3Yl",
        "colab_type": "text"
      },
      "source": [
        "Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434eK6oopyyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import itertools\n",
        "import sys\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.autograd import Variable\n",
        "import torchvision.datasets as dset\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCcT8YI7p-K0",
        "colab_type": "text"
      },
      "source": [
        "Define Feature Extractor, Generator, and Discriminator Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Ji9L3_gccv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        vgg19_model = vgg19(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.feature_extractor(img)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        # First layer\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n",
        "\n",
        "        # Residual blocks\n",
        "        res_blocks = []\n",
        "        for _ in range(n_residual_blocks):\n",
        "            res_blocks.append(ResidualBlock(64))\n",
        "        self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "        # Second conv layer post residual blocks\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n",
        "\n",
        "        # Upsampling layers\n",
        "        upsampling = []\n",
        "        for out_features in range(2):\n",
        "            upsampling += [\n",
        "                # nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(64, 256, 3, 1, 1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.PixelShuffle(upscale_factor=2),\n",
        "                nn.PReLU(),\n",
        "            ]\n",
        "        self.upsampling = nn.Sequential(*upsampling)\n",
        "\n",
        "        # Final output layer\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        out = self.res_blocks(out1)\n",
        "        out2 = self.conv2(out)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        in_channels, in_height, in_width = self.input_shape\n",
        "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
        "        self.output_shape = (1, patch_h, patch_w)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "            layers = []\n",
        "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
        "            if not first_block:\n",
        "                layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "            layers.append(nn.BatchNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = in_channels\n",
        "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqv3LZhYuErr",
        "colab_type": "text"
      },
      "source": [
        "Create class for dataset which overrides the dataset class. This class will let me divide the CelebA dataset images into low resolution images and high resolution images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI5IC3RqglIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalization parameters for pre-trained PyTorch models\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, hr_shape):\n",
        "        hr_height, hr_width = hr_shape\n",
        "        # Transforms for low resolution images and high resolution images\n",
        "        self.lr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        self.hr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        img_lr = self.lr_transform(img)\n",
        "        img_hr = self.hr_transform(img)\n",
        "\n",
        "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrTyK6iGwCwT",
        "colab_type": "text"
      },
      "source": [
        "Make folders to save data and define variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ZHuoyBwAEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs(\"images\", exist_ok=True)\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "epoch=0\n",
        "n_epochs=100\n",
        "root = 'data_faces/img_align_celeba'\n",
        "dataset_name=\"img_align_celeba\"\n",
        "batch_size=128\n",
        "lr=0.0002\n",
        "b1=0.5\n",
        "b2=0.999\n",
        "decay_epoch=100\n",
        "n_cpu=8\n",
        "hr_height = 64\n",
        "hr_width = 64\n",
        "channels=3\n",
        "sample_interval=100\n",
        "checkpoint_interval=-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GjZPqonvnNf",
        "colab_type": "text"
      },
      "source": [
        "Initialize Generator and Discriminator \n",
        "\n",
        "Define Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaODE4qRgYyB",
        "colab_type": "code",
        "outputId": "5152086a-866c-4520-ebab-aca559badcd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "hr_shape = (hr_height, hr_width)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = GeneratorResNet()\n",
        "discriminator = Discriminator(input_shape=(channels, *hr_shape))\n",
        "feature_extractor = FeatureExtractor()\n",
        "\n",
        "# Set feature extractor to inference mode\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Losses\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_content = torch.nn.L1Loss()\n",
        "\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    feature_extractor = feature_extractor.cuda()\n",
        "    criterion_GAN = criterion_GAN.cuda()\n",
        "    criterion_content = criterion_content.cuda()\n",
        "\n",
        "if epoch != 0:\n",
        "    # Load pretrained models\n",
        "    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"))\n",
        "    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"))\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:07<00:00, 73.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me29_Jsh1aAW",
        "colab_type": "text"
      },
      "source": [
        "Download CelebA dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWj3P6verQxW",
        "colab_type": "code",
        "outputId": "0543910b-9ef6-457d-ed40-1fe302f92ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip \n",
        "\n",
        "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"data_faces/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-15 09:47:05--  https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.116.144\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.116.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1443490838 (1.3G) [application/zip]\n",
            "Saving to: ‘celeba.zip’\n",
            "\n",
            "celeba.zip          100%[===================>]   1.34G  42.8MB/s    in 32s     \n",
            "\n",
            "2019-12-15 09:47:37 (42.5 MB/s) - ‘celeba.zip’ saved [1443490838/1443490838]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI3vf2k52KKy",
        "colab_type": "text"
      },
      "source": [
        "I used this code to delete some of the image files from the dataset to make it smaller so that the training time would decrease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U10gWcSmJcT",
        "colab_type": "code",
        "outputId": "690f3750-bec1-494a-be9c-2d2811bec34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "\n",
        "files = os.listdir(root)  # Get filenames in current folder\n",
        "files = random.sample(files, 18000)  # Pick 500 random files\n",
        "for file in files:  # Go over each file name to be deleted\n",
        "    f = os.path.join(root, file)  # Create valid path to file\n",
        "    os.remove(f)  # Remove the file\n",
        "\n",
        "img_list = os.listdir(root)\n",
        "print(len(img_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "184599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vylC0ZqDC7Ea",
        "colab_type": "text"
      },
      "source": [
        "Create the dataloader for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNs5cXdMLEWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgdataset = ImageDataset(root, hr_shape=hr_shape)\n",
        "\n",
        "# Create the dataloader\n",
        "dataloader = DataLoader(\n",
        "    imgdataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8tQNP1x2l-U",
        "colab_type": "text"
      },
      "source": [
        "Mounting my drive to google colab so that I can save the output images and models to the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCAYJUNsa2Qj",
        "colab_type": "code",
        "outputId": "102594fc-286a-4c9f-e3ba-39c965ad312a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LhCCHm029LO",
        "colab_type": "text"
      },
      "source": [
        "Make folders in google drive to save the images and models in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m42nbxnbU-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!mkdir -p \"/content/gdrive/My Drive/images\"\n",
        "!mkdir -p \"/content/gdrive/My Drive/saved_models\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPS8IF7f5OEm",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFtoEsPpmgxq",
        "colab_type": "code",
        "outputId": "3bdad1ab-0ff2-4ac0-de71-e39b380277d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(epoch, n_epochs):\n",
        "    for i, imgs in enumerate(dataloader):\n",
        "\n",
        "        # Configure model input\n",
        "        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n",
        "        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n",
        "\n",
        "        # ------------------\n",
        "        #  Train Generators\n",
        "        # ------------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generate a high resolution image from low resolution input\n",
        "        gen_hr = generator(imgs_lr)\n",
        "\n",
        "        # Adversarial loss\n",
        "        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n",
        "\n",
        "        # Content loss\n",
        "        gen_features = feature_extractor(gen_hr)\n",
        "        real_features = feature_extractor(imgs_hr)\n",
        "        loss_content = criterion_content(gen_features, real_features.detach())\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_content + 1e-3 * loss_GAN\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss of real and fake images\n",
        "        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n",
        "        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(loss_G.item())\n",
        "        D_losses.append(loss_D.item())\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "        if i % 50 == 0:\n",
        "          print(\n",
        "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "              % (epoch, n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n",
        "          )\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % sample_interval == 0:\n",
        "            # Save image grid with upsampled inputs and SRGAN outputs\n",
        "            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
        "            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n",
        "            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n",
        "            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n",
        "            save_image(img_grid, \"/content/gdrive/My Drive/images/%d.png\" % batches_done, normalize=False)\n",
        "\n",
        "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
        "        # Save model checkpoints\n",
        "        torch.save(generator.state_dict(), \"/content/gdrive/My Drive/saved_models/generator_%d.pth\" % epoch)\n",
        "        torch.save(discriminator.state_dict(), \"/content/gdrive/My Drive/saved_models/discriminator_%d.pth\" % epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/100] [Batch 0/1443] [D loss: 0.792671] [G loss: 2.802144]\n",
            "[Epoch 0/100] [Batch 50/1443] [D loss: 0.043405] [G loss: 2.287370]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy5bCsUjDCvx",
        "colab_type": "text"
      },
      "source": [
        "Plotting graph to show generator and discriminator loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOnmkAaqOvNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPljIWsHDSHX",
        "colab_type": "text"
      },
      "source": [
        "Run generator model over the input image and display results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykEsOqrn8oib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage import io\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "img_file = \"\"\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  img_file = fn\n",
        "\n",
        "#img_file = \"16x16-obama.jpg\"\n",
        "img_lr = io.imread(img_file)\n",
        "\n",
        "generator = generator.cuda()\n",
        "input_img =torch.tensor(img_lr).float()\n",
        "input_img = Variable(input_img)\n",
        "input_img = np.transpose(input_img, (2, 0, 1))\n",
        "input_img= input_img.unsqueeze(0)\n",
        "input_img= input_img.cuda()\n",
        "# Generate a high resolution image from low resolution input\n",
        "gen_hr = generator(input_img)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Image\")\n",
        "plt.imshow(img_lr)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Image\")\n",
        "plt.imshow(np.transpose(gen_hr[0].detach().cpu(), (1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}